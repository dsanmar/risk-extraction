{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a71914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sec-edgar-downloader\n",
      "  Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pyrate-limiter>=3.6.0 (from sec-edgar-downloader)\n",
      "  Downloading pyrate_limiter-3.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl (14 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Downloading pyrate_limiter-3.7.1-py3-none-any.whl (28 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, soupsieve, pyrate-limiter, idna, charset_normalizer, certifi, requests, beautifulsoup4, sec-edgar-downloader\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/10\u001b[0m [sec-edgar-downloader]utifulsoup4]\n",
      "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.13.4 certifi-2025.6.15 charset_normalizer-3.4.2 idna-3.10 pyrate-limiter-3.7.1 requests-2.32.4 sec-edgar-downloader-5.0.3 soupsieve-2.7 typing-extensions-4.14.0 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sec-edgar-downloader beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62308ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Looking for 10-K filing for EFX\n",
      "âŒ Error for EFX 10-K: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 10-K filing for GATX\n",
      "âŒ Error for GATX 10-K: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 10-K filing for LGF-A\n",
      "âŒ Error for LGF-A 10-K: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 10-K filing for ACIW\n",
      "âŒ Error for ACIW 10-K: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 10-K filing for HSII\n",
      "âŒ Error for HSII 10-K: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 10-K filing for FIX\n",
      "âŒ Error for FIX 10-K: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 10-K filing for CVI\n",
      "âŒ Error for CVI 10-K: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 20-F filing for CYD\n",
      "âŒ Error for CYD 20-F: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 20-F filing for FME\n",
      "âŒ Error for FME 20-F: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 20-F filing for NVGS\n",
      "âŒ Error for NVGS 20-F: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 20-F filing for SQNS\n",
      "âŒ Error for SQNS 20-F: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 20-F filing for AER\n",
      "âŒ Error for AER 20-F: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 20-F filing for VIVT3\n",
      "âŒ Error for VIVT3 20-F: Downloader.get() takes 3 positional arguments but 4 were given\n",
      "ğŸ” Looking for 20-F filing for SOL\n",
      "âŒ Error for SOL 20-F: Downloader.get() takes 3 positional arguments but 4 were given\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sec_edgar_downloader import Downloader\n",
    "from datetime import datetime\n",
    "\n",
    "# Your output folder\n",
    "DATA_DIR = \"/Users/marsanto/DevProjects/python/risk-extraction/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Your company lists\n",
    "ten_k_tickers = [\"EFX\", \"GATX\", \"LGF-A\", \"ACIW\", \"HSII\", \"FIX\", \"CVI\"]\n",
    "twenty_f_tickers = [\"CYD\", \"FME\", \"NVGS\", \"SQNS\", \"AER\", \"VIVT3\", \"SOL\"]\n",
    "\n",
    "# Instantiate downloader with a valid email\n",
    "dl = Downloader(\"mariadev\", \"santosmariag01@gmail.com\")  # Replace with your email\n",
    "\n",
    "def download_first_pdf_from_filing(ticker, form_type):\n",
    "    print(f\"ğŸ” Looking for {form_type} filing for {ticker}\")\n",
    "    try:\n",
    "        dl.get(\"10-K\", \"EFX\", 1)\n",
    "        # Get download path\n",
    "        path = os.path.join(dl._Downloader__save_directory, ticker, form_type)\n",
    "        latest_filing = sorted(os.listdir(path), reverse=True)[0]\n",
    "        full_path = os.path.join(path, latest_filing)\n",
    "\n",
    "        # Parse the index.html to find PDF link\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "        links = soup.find_all(\"a\")\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if href and href.endswith(\".pdf\"):\n",
    "                pdf_url = \"https://www.sec.gov\" + href\n",
    "                filename = f\"{ticker}_{form_type}_{datetime.now().strftime('%Y%m%d')}.pdf\"\n",
    "                file_path = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "                # Download the PDF\n",
    "                response = requests.get(pdf_url)\n",
    "                with open(file_path, \"wb\") as out:\n",
    "                    out.write(response.content)\n",
    "                print(f\"âœ… Downloaded: {filename}\")\n",
    "                return\n",
    "\n",
    "        print(f\"No PDF found in {ticker} {form_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error for {ticker} {form_type}: {e}\")\n",
    "\n",
    "# Loop through both sets\n",
    "for ticker in ten_k_tickers:\n",
    "    download_first_pdf_from_filing(ticker, \"10-K\")\n",
    "\n",
    "for ticker in twenty_f_tickers:\n",
    "    download_first_pdf_from_filing(ticker, \"20-F\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
